{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9648c5-fb04-4ec4-95b7-5ac4c6fc885a",
   "metadata": {},
   "source": [
    "# <font color='blue'>Introduction to PySpark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0f2b9-4211-4e8e-80ea-ebd433d1052d",
   "metadata": {},
   "source": [
    "- In this Notebook, we will learn how to use and operate Spark using Python called PySpark\n",
    "- To make sure PySpark is already installed on your machine, we can import `pyspark` and check the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5919c32-7c6a-473f-8849-89b5fb32161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d40410b-2247-4332-91f5-d18d30254422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check pyspark version\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2b176-98ce-457f-bed9-169d34ddcc52",
   "metadata": {},
   "source": [
    "- Ok, we're already importing `pyspark` then how do we use the function and component inside pyspark?\n",
    "- To use all the functions and components inside pyspark, we must create `SparkSession`\n",
    "- To create `SparkSession` we can import this function `from pyspark.sql import SparkSession`\n",
    "- Then, if we want to create `SparkSession` we can use this code snippet\n",
    "  \n",
    "```python\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"...\") \\ # you can insert what kind of appName that you want\n",
    "    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5e57d0-ff51-49d8-8f38-1fed86f2e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee6f58e-9492-476b-99cf-6a1cdc29cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Learn PySpark Data Pipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f67393-8981-44cb-88ef-e624856412c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Learn PySpark Data Pipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd708a7b090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3b052-a32e-4b36-9067-27c1465012ee",
   "metadata": {},
   "source": [
    "### SparkContext UI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06f856-b2b4-4089-acff-df519966e8c1",
   "metadata": {},
   "source": [
    "- When we're creating SparkContext, we can get a web-based User Interface that will give us information about Spark execution, like run time, the process being split into many tasks, etc\n",
    "- It will help us to debug, check our Spark performance, and many more\n",
    "- To access SparkContext UI, we can access http://localhost:4040\n",
    "- Make sure, to access SparkContext UI you're already create SparkSession\n",
    "- If you're already creating SparkSession, you can click button `Spark UI`\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/spark_ui.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b614e99-2705-4c99-956e-159e47e784d9",
   "metadata": {},
   "source": [
    "You can try to run this code below and see the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a68ad19-cedb-49cb-a7ca-70f5963fe49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    1|    2|    3|a b c|\n",
      "|    4|    5|    6|d e f|\n",
      "|    7|    8|    9|g h i|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd = spark.sparkContext\\\n",
    "            .parallelize([(1, 2, 3, 'a b c'),\n",
    "                            (4, 5, 6, 'd e f'),\n",
    "                            (7, 8, 9, 'g h i')])\\\n",
    "            .toDF(['col_1', 'col_2', 'col_3', 'col_4'])\n",
    "\n",
    "df_rdd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b8a94-4c94-4582-bbf2-6d0f86fab0fe",
   "metadata": {},
   "source": [
    "In Spark UI will show what jobs are currently run by PySpark\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/spark_ui_run.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53a50e-0e84-44bb-8095-8584fa2a3cfc",
   "metadata": {},
   "source": [
    "# <font color='blue'>Pandas vs PySpark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a27fcb-ce9a-4160-867c-89465143911b",
   "metadata": {},
   "source": [
    "- In the previous section, we already learned that PySpark is similar to Pandas \n",
    "- Especially for data processing, we can:\n",
    "    - Read Data\n",
    "    - Select Data\n",
    "    - Filter Data\n",
    "    - Transform Data\n",
    "    - etc\n",
    "- We're going to learn about how to do it using PySpark in this course\n",
    "- But what's the difference between them?\n",
    "- The main difference between them is that PySpark will split the task!\n",
    "- It will affect in speed time when reading data\n",
    "- In this section, we're going to demonstrate it to see the speed difference\n",
    "- You will see the difference if using data that have many records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88583ebc-0eff-4135-9981-63b07d01992f",
   "metadata": {},
   "source": [
    "---\n",
    "- We will try to read data that have more than 5 million rows\n",
    "- Say we have transaction data about Flights data, you can access the data in here [dataset](https://drive.google.com/file/d/1GggQUGzWFeLXwITGKN0s_NNmkep9V-WQ/view?usp=drive_link)\n",
    "- We will compare it with Pandas and PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4af7d7-9f05-4754-97a7-79c6699b5021",
   "metadata": {},
   "source": [
    "**Pandas**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eacb956a-6d4a-494e-86e5-23d6ec1296e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002a015e-a016-4704-8acf-26349c90fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3fa421b-1b28-4c75-9d48-3d626aeaf638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to compare the speed \n",
    "\n",
    "def read_big_table_pandas(filename: str):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = pd.read_csv(DATA_PATH + filename)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return elapsed_time, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f960852-846f-4c2f-bc74-580ffff88e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to read CSV: 0.04840540885925293 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed_time, df_pandas = read_big_table_pandas(filename = \"bank_churners.csv\")\n",
    "print(f\"Time to read CSV: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eba2c4bc-f7a5-4b68-ab2d-72eb244645bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attrition_Flag</th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Dependent_count</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income_Category</th>\n",
       "      <th>Card_Category</th>\n",
       "      <th>Months_on_book</th>\n",
       "      <th>Total_Relationship_Count</th>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <th>Contacts_Count_12_mon</th>\n",
       "      <th>Credit_Limit</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>Avg_Open_To_Buy</th>\n",
       "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
       "      <th>Total_Trans_Amt</th>\n",
       "      <th>Total_Trans_Ct</th>\n",
       "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
       "      <th>Avg_Utilization_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>45</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>High School</td>\n",
       "      <td>Married</td>\n",
       "      <td>$60K - $80K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12691.0</td>\n",
       "      <td>777</td>\n",
       "      <td>11914.0</td>\n",
       "      <td>1.335</td>\n",
       "      <td>1144</td>\n",
       "      <td>42</td>\n",
       "      <td>1.625</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Single</td>\n",
       "      <td>Less than $40K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8256.0</td>\n",
       "      <td>864</td>\n",
       "      <td>7392.0</td>\n",
       "      <td>1.541</td>\n",
       "      <td>1291</td>\n",
       "      <td>33</td>\n",
       "      <td>3.714</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Married</td>\n",
       "      <td>$80K - $120K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>2.594</td>\n",
       "      <td>1887</td>\n",
       "      <td>20</td>\n",
       "      <td>2.333</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>High School</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Less than $40K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3313.0</td>\n",
       "      <td>2517</td>\n",
       "      <td>796.0</td>\n",
       "      <td>1.405</td>\n",
       "      <td>1171</td>\n",
       "      <td>20</td>\n",
       "      <td>2.333</td>\n",
       "      <td>0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>Uneducated</td>\n",
       "      <td>Married</td>\n",
       "      <td>$60K - $80K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4716.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4716.0</td>\n",
       "      <td>2.175</td>\n",
       "      <td>816</td>\n",
       "      <td>28</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Attrition_Flag  Customer_Age Gender  Dependent_count Education_Level  \\\n",
       "0  Existing Customer            45      M                3     High School   \n",
       "1  Existing Customer            49      F                5        Graduate   \n",
       "2  Existing Customer            51      M                3        Graduate   \n",
       "3  Existing Customer            40      F                4     High School   \n",
       "4  Existing Customer            40      M                3      Uneducated   \n",
       "\n",
       "  Marital_Status Income_Category Card_Category  Months_on_book  \\\n",
       "0        Married     $60K - $80K          Blue              39   \n",
       "1         Single  Less than $40K          Blue              44   \n",
       "2        Married    $80K - $120K          Blue              36   \n",
       "3        Unknown  Less than $40K          Blue              34   \n",
       "4        Married     $60K - $80K          Blue              21   \n",
       "\n",
       "   Total_Relationship_Count  Months_Inactive_12_mon  Contacts_Count_12_mon  \\\n",
       "0                         5                       1                      3   \n",
       "1                         6                       1                      2   \n",
       "2                         4                       1                      0   \n",
       "3                         3                       4                      1   \n",
       "4                         5                       1                      0   \n",
       "\n",
       "   Credit_Limit  Total_Revolving_Bal  Avg_Open_To_Buy  Total_Amt_Chng_Q4_Q1  \\\n",
       "0       12691.0                  777          11914.0                 1.335   \n",
       "1        8256.0                  864           7392.0                 1.541   \n",
       "2        3418.0                    0           3418.0                 2.594   \n",
       "3        3313.0                 2517            796.0                 1.405   \n",
       "4        4716.0                    0           4716.0                 2.175   \n",
       "\n",
       "   Total_Trans_Amt  Total_Trans_Ct  Total_Ct_Chng_Q4_Q1  Avg_Utilization_Ratio  \n",
       "0             1144              42                1.625                  0.061  \n",
       "1             1291              33                3.714                  0.105  \n",
       "2             1887              20                2.333                  0.000  \n",
       "3             1171              20                2.333                  0.760  \n",
       "4              816              28                2.500                  0.000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75670484-0187-4b52-bfaf-fdd28803301b",
   "metadata": {},
   "source": [
    "**PySpark**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d09594a-3819-46d3-bcb3-7e30a1e14717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_big_table_pyspark(filename: str):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = spark \\\n",
    "        .read \\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .csv(DATA_PATH + filename)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return elapsed_time, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f08d1f5-86d5-415c-8834-660a95f31ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to read CSV: 0.3867511749267578 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed_time, df_pyspark = read_big_table_pyspark(filename = \"bank_churners.csv\")\n",
    "print(f\"Time to read CSV: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "158fff66-b92d-4022-839b-7a0b30e915c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|Existing Customer|          45|     M|              3|    High School|       Married|    $60K - $80K|         Blue|            39|                       5|                     1|                    3|       12691|                777|          11914|               1.335|           1144|            42|              1.625|                0.061|\n",
      "|Existing Customer|          49|     F|              5|       Graduate|        Single| Less than $40K|         Blue|            44|                       6|                     1|                    2|        8256|                864|           7392|               1.541|           1291|            33|              3.714|                0.105|\n",
      "|Existing Customer|          51|     M|              3|       Graduate|       Married|   $80K - $120K|         Blue|            36|                       4|                     1|                    0|        3418|                  0|           3418|               2.594|           1887|            20|              2.333|                    0|\n",
      "|Existing Customer|          40|     F|              4|    High School|       Unknown| Less than $40K|         Blue|            34|                       3|                     4|                    1|        3313|               2517|            796|               1.405|           1171|            20|              2.333|                 0.76|\n",
      "|Existing Customer|          40|     M|              3|     Uneducated|       Married|    $60K - $80K|         Blue|            21|                       5|                     1|                    0|        4716|                  0|           4716|               2.175|            816|            28|                2.5|                    0|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f42f65-7cc2-4858-8d9c-c73eb0791312",
   "metadata": {},
   "source": [
    "# <font color='blue'>Read Data using PySpark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ca954-f0bc-4c2c-9504-590b4ce91cea",
   "metadata": {},
   "source": [
    "- Just like in Pandas, when using PySpark we can read data from various formats like:\n",
    "    - File\n",
    "    - Database\n",
    "    - JSON\n",
    "    - API\n",
    "    - etc\n",
    "- For more detailed what kind of format data that PySpark can read, you can check in this [documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html)\n",
    "- PySpark also can treat the data into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36331f-3c4c-4f92-9ee7-35309d3d33e2",
   "metadata": {},
   "source": [
    "---\n",
    "In this section, we're going to learn how to read data from csv file and Postgres database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb7876-c43b-417b-b5f3-db8e29b0149d",
   "metadata": {},
   "source": [
    "**Read csv file**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e025a2-7763-4a74-99e5-ea002c60c36c",
   "metadata": {},
   "source": [
    "- Say, we're using Flights Transactional Data\n",
    "- The file in directory `data/bank_churners.csv`\n",
    "- To read data using PySpark, first we can use `SparkSession` that we've already created\n",
    "- So, for the next of this course we will use the variable `spark`\n",
    "- The snippet code will be like this\n",
    "\n",
    "```python\n",
    "df = spark.read.option(\"header\", \"true\").FORMAT_DATA(...) \n",
    "```\n",
    "\n",
    "- In `FORMAT_DATA`we can adjust with the format data that we have\n",
    "- So, in this case if we're going to read the `csv` file the `FORMAT_DATA` will be `csv`\n",
    "- Then, inside `csv` we can put the filename of the data\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(filename)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b73ce7-8de6-4146-8f80-73f9c4675dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"../data/bank_churners.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc1f90e-6d7b-4882-afe6-75cffc8092df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|Existing Customer|          45|     M|              3|    High School|       Married|    $60K - $80K|         Blue|            39|                       5|                     1|                    3|       12691|                777|          11914|               1.335|           1144|            42|              1.625|                0.061|\n",
      "|Existing Customer|          49|     F|              5|       Graduate|        Single| Less than $40K|         Blue|            44|                       6|                     1|                    2|        8256|                864|           7392|               1.541|           1291|            33|              3.714|                0.105|\n",
      "|Existing Customer|          51|     M|              3|       Graduate|       Married|   $80K - $120K|         Blue|            36|                       4|                     1|                    0|        3418|                  0|           3418|               2.594|           1887|            20|              2.333|                    0|\n",
      "|Existing Customer|          40|     F|              4|    High School|       Unknown| Less than $40K|         Blue|            34|                       3|                     4|                    1|        3313|               2517|            796|               1.405|           1171|            20|              2.333|                 0.76|\n",
      "|Existing Customer|          40|     M|              3|     Uneducated|       Married|    $60K - $80K|         Blue|            21|                       5|                     1|                    0|        4716|                  0|           4716|               2.175|            816|            28|                2.5|                    0|\n",
      "|Existing Customer|          44|     M|              2|       Graduate|       Married|    $40K - $60K|         Blue|            36|                       3|                     1|                    2|        4010|               1247|           2763|               1.376|           1088|            24|              0.846|                0.311|\n",
      "|Existing Customer|          32|     M|              0|    High School|       Unknown|    $60K - $80K|       Silver|            27|                       2|                     2|                    2|       29081|               1396|          27685|               2.204|           1538|            36|              0.714|                0.048|\n",
      "|Existing Customer|          37|     M|              3|     Uneducated|        Single|    $60K - $80K|         Blue|            36|                       5|                     2|                    0|       22352|               2517|          19835|               3.355|           1350|            24|              1.182|                0.113|\n",
      "|Existing Customer|          48|     M|              2|           NULL|        Single|   $80K - $120K|         Blue|            36|                       6|                     3|                    3|       11656|               1677|           9979|               1.524|           1441|            32|              0.882|                0.144|\n",
      "|Existing Customer|          65|     M|              1|        Unknown|       Married|    $40K - $60K|         Blue|            54|                       6|                     2|                    3|        9095|               1587|           7508|               1.433|           1314|            26|              1.364|                0.174|\n",
      "|Existing Customer|          56|     M|              1|        College|        Single|   $80K - $120K|         Blue|            36|                       3|                     6|                    0|       11751|                  0|          11751|               3.397|           1539|            17|               3.25|                    0|\n",
      "|Existing Customer|          35|     M|              3|       Graduate|       Unknown|    $60K - $80K|         Blue|            30|                       5|                     1|                    3|        8547|               1666|           6881|               1.163|           1311|            33|                  2|                0.195|\n",
      "|Existing Customer|          57|     F|              2|       Graduate|       Married| Less than $40K|         Blue|            48|                       5|                     2|                    2|        2436|                680|           1756|                1.19|           1570|            29|              0.611|                0.279|\n",
      "|Existing Customer|          44|     M|              4|        Unknown|       Unknown|   $80K - $120K|         Blue|            37|                       5|                     1|                    2|        4234|                972|           3262|               1.707|           1348|            27|                1.7|                 0.23|\n",
      "|Existing Customer|          48|     M|              4|  Post-Graduate|        Single|   $80K - $120K|         Blue|            36|                       6|                     2|                    3|       30367|               2362|          28005|               1.708|           1671|            27|              0.929|                0.078|\n",
      "|Existing Customer|          41|     M|              3|        Unknown|       Married|   $80K - $120K|         Blue|            34|                       4|                     4|                    1|       13535|               1291|          12244|               0.653|           1028|            21|              1.625|                0.095|\n",
      "|Existing Customer|          61|     M|              1|    High School|       Married|    $40K - $60K|         Blue|            56|                       2|                     2|                    3|        3193|               2517|            676|               1.831|           1336|            30|              1.143|                0.788|\n",
      "|Existing Customer|          45|     F|              2|       Graduate|       Married|        Unknown|         Blue|            37|                       6|                     1|                    2|       14470|               1157|          13313|               0.966|           1207|            21|              0.909|                 0.08|\n",
      "|Existing Customer|          47|     M|              1|      Doctorate|      Divorced|    $60K - $80K|         Blue|            42|                       5|                     2|                    0|       20979|               1800|          19179|               0.906|           1178|            27|              0.929|                0.086|\n",
      "|Attrited Customer|          62|     F|              0|       Graduate|       Married| Less than $40K|         Blue|            49|                       2|                     3|                    3|      1438.3|                  0|         1438.3|               1.047|            692|            16|                0.6|                    0|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edff07-27f9-46bb-941c-199967b6e3ce",
   "metadata": {},
   "source": [
    "**Read Database data**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051eaa0-2812-47ea-9f3d-7818dfe8e6ef",
   "metadata": {},
   "source": [
    "- To read data from the database, first we must own the database driver\n",
    "- Because PySpark is run in Scala and Java, so our database driver must be in `jar` files\n",
    "- If you're using PySpark Docker from Pacmann's repo, we've already included `jar` files in that repo\n",
    "- Next step, we can copy the `jar` files into our docker container by running this command in the terminal\n",
    "  \n",
    "  `docker cp driver/postgresql-42.6.0.jar pyspark_container:/usr/local/spark/jars/postgresql-42.6.0.jar`\n",
    "\n",
    "- After we set up the database driver, we can start reading the database by using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833727a-52db-4319-98d6-28a8eb133cbe",
   "metadata": {},
   "source": [
    "---\n",
    "- Say, we're already have a database from our docker compose and this is our config for database and we want to read `payments` table\n",
    "\n",
    "```\n",
    "DB_URL = jdbc:postgresql://pachotel_db_container:5432/pachotel\n",
    "DB_TABLE = payment\n",
    "DB_USER = postgres\n",
    "DB_PASS = cobapassword\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc3a344-092f-433b-8724-9f40a19c94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable for database\n",
    "\n",
    "DB_URL = \"jdbc:postgresql://pachotel_db_container:5432/pachotel\"\n",
    "DB_TABLE = \"payment\" \n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"cobapassword\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d2330b-9f70-43fb-9a29-4a101a9c18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config\n",
    "jdbc_url = DB_URL\n",
    "table_name = DB_TABLE\n",
    "connection_properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASS,\n",
    "    \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b1860-c53d-41b4-a2d6-c0646e916362",
   "metadata": {},
   "source": [
    "- To read the database using PySpark, we can use `jdbc`\n",
    "- Then, we set parameters for:\n",
    "    - `url`: URL for database\n",
    "    - `table`: table name that we want to access\n",
    "    - `properties`: database config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9acdd7cf-821c-453a-87f4-d189b65ed0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------+--------------+--------------+--------------------+--------------------+--------------------+\n",
      "|payment_id|reservation_id|provider|account_number|payment_status|        payment_date|         expire_date|          created_at|\n",
      "+----------+--------------+--------+--------------+--------------+--------------------+--------------------+--------------------+\n",
      "|         1|             1|     Ovo|     038137149|       Success|2020-10-20 17:07:...|2020-10-28 18:10:...|2024-02-09 13:59:...|\n",
      "|         2|             2|     BCA|     042103729|       Success|2015-02-28 16:44:...|2015-03-08 05:27:...|2024-02-09 13:59:...|\n",
      "|         3|             3| Permata|     058689635|       Success|2022-04-03 13:27:...|2022-04-06 14:20:...|2024-02-09 13:59:...|\n",
      "|         4|             4|     BNI|     107161965|       Success|2019-10-04 08:39:...|2019-10-08 02:01:...|2024-02-09 13:59:...|\n",
      "|         5|             5|     BSI|     014334131|        Failed|                NULL|2020-06-14 12:02:...|2024-02-09 13:59:...|\n",
      "|         6|             6|     Ovo|     043583173|       Waiting|                NULL|2020-03-25 03:35:...|2024-02-09 13:59:...|\n",
      "|         7|             7|     BSI|     021381685|       Waiting|                NULL|2017-10-12 03:15:...|2024-02-09 13:59:...|\n",
      "|         8|             8| Permata|     119636925|       Success|2019-07-06 00:27:...|2019-07-16 05:38:...|2024-02-09 13:59:...|\n",
      "|         9|             9| Mandiri|     128447299|       Success|2020-10-22 21:58:...|2020-11-07 21:05:...|2024-02-09 13:59:...|\n",
      "|        10|            10|     BCA|     021032743|        Failed|                NULL|2021-03-23 15:28:...|2024-02-09 13:59:...|\n",
      "|        11|            11|     BCA|     016173439|       Success|2015-12-30 19:38:...|2016-01-04 23:05:...|2024-02-09 13:59:...|\n",
      "|        12|            12| Mandiri|     104632264|       Success|2016-04-20 11:00:...|2016-04-29 02:21:...|2024-02-09 13:59:...|\n",
      "|        13|            13|     BSI|     043534397|        Failed|                NULL|2021-03-09 03:35:...|2024-02-09 13:59:...|\n",
      "|        14|            14|     BNI|     049889648|       Success|2015-08-30 02:50:...|2015-09-06 15:20:...|2024-02-09 13:59:...|\n",
      "|        15|            15|     Ovo|     018430413|       Success|2015-08-25 14:16:...|2015-09-11 10:50:...|2024-02-09 13:59:...|\n",
      "|        16|            16|     BSI|     050658732|       Success|2015-06-20 02:46:...|2015-07-14 03:01:...|2024-02-09 13:59:...|\n",
      "|        17|            17| Permata|     127295361|       Waiting|                NULL|2017-07-04 16:06:...|2024-02-09 13:59:...|\n",
      "|        18|            18|     Ovo|     017623865|        Failed|                NULL|2022-02-07 12:28:...|2024-02-09 13:59:...|\n",
      "|        19|            19| Permata|     045720659|       Success|2020-08-13 21:31:...|2020-08-18 00:19:...|2024-02-09 13:59:...|\n",
      "|        20|            20|     BRI|     024522616|       Success|2019-04-29 02:30:...|2019-05-04 07:50:...|2024-02-09 13:59:...|\n",
      "+----------+--------------+--------+--------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=connection_properties)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9bd27d-c50f-4ec9-9198-1dcc2b7278c6",
   "metadata": {},
   "source": [
    "- We've already select and filtered data based on the requirements given to us\n",
    "- The next step is to **store the results** or export the data\n",
    "- By using PySpark, we can save the output into a file or database\n",
    "- When export the data we can use method `write` then followed by format data we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78ebee-0851-4260-8cc2-89b33ecf74ce",
   "metadata": {},
   "source": [
    "**Save to File**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed14b3-a60d-4ad9-a4b3-a3547db38ecd",
   "metadata": {},
   "source": [
    "- In this case, we will try to save the output into CSV files\n",
    "- When exporting the file into CSV by using PySpark, we can choose the output format:\n",
    "    - can be only one file: `output.csv`\n",
    "    - split the output: `output_1.csv`, `output_2.csv`, `output_n.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9da037-643c-476f-92ac-e12182b31c76",
   "metadata": {},
   "source": [
    "---\n",
    "- Given the flight dataset, we're going to selecting and filtering the data\n",
    "- Selected columns:\n",
    "    - `ACCOUNT_NUMBER`\n",
    "    - `PAYMENT_STATUS`\n",
    "    - `PAYMENT_DATE`\n",
    "    - `EXPIRE_DATE`\n",
    "    - `PROVIDER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2b07d74-8eca-4cb5-a0c0-96e082ef5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_COLS = [\"ACCOUNT_NUMBER\", \"PAYMENT_STATUS\", \"PAYMENT_DATE\",\"EXPIRE_DATE\",\"PROVIDER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30b89e43-cb9d-4077-aef9-9fc371562aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------+--------------------+--------+\n",
      "|ACCOUNT_NUMBER|PAYMENT_STATUS|        PAYMENT_DATE|         EXPIRE_DATE|PROVIDER|\n",
      "+--------------+--------------+--------------------+--------------------+--------+\n",
      "|     038137149|       Success|2020-10-20 17:07:...|2020-10-28 18:10:...|     Ovo|\n",
      "|     042103729|       Success|2015-02-28 16:44:...|2015-03-08 05:27:...|     BCA|\n",
      "|     058689635|       Success|2022-04-03 13:27:...|2022-04-06 14:20:...| Permata|\n",
      "|     107161965|       Success|2019-10-04 08:39:...|2019-10-08 02:01:...|     BNI|\n",
      "|     014334131|        Failed|                NULL|2020-06-14 12:02:...|     BSI|\n",
      "|     043583173|       Waiting|                NULL|2020-03-25 03:35:...|     Ovo|\n",
      "|     021381685|       Waiting|                NULL|2017-10-12 03:15:...|     BSI|\n",
      "|     119636925|       Success|2019-07-06 00:27:...|2019-07-16 05:38:...| Permata|\n",
      "|     128447299|       Success|2020-10-22 21:58:...|2020-11-07 21:05:...| Mandiri|\n",
      "|     021032743|        Failed|                NULL|2021-03-23 15:28:...|     BCA|\n",
      "|     016173439|       Success|2015-12-30 19:38:...|2016-01-04 23:05:...|     BCA|\n",
      "|     104632264|       Success|2016-04-20 11:00:...|2016-04-29 02:21:...| Mandiri|\n",
      "|     043534397|        Failed|                NULL|2021-03-09 03:35:...|     BSI|\n",
      "|     049889648|       Success|2015-08-30 02:50:...|2015-09-06 15:20:...|     BNI|\n",
      "|     018430413|       Success|2015-08-25 14:16:...|2015-09-11 10:50:...|     Ovo|\n",
      "|     050658732|       Success|2015-06-20 02:46:...|2015-07-14 03:01:...|     BSI|\n",
      "|     127295361|       Waiting|                NULL|2017-07-04 16:06:...| Permata|\n",
      "|     017623865|        Failed|                NULL|2022-02-07 12:28:...|     Ovo|\n",
      "|     045720659|       Success|2020-08-13 21:31:...|2020-08-18 00:19:...| Permata|\n",
      "|     024522616|       Success|2019-04-29 02:30:...|2019-05-04 07:50:...|     BRI|\n",
      "+--------------+--------------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.select(SELECTED_COLS)\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9d88f-2da0-4bd4-95c7-3d82a1a89780",
   "metadata": {},
   "source": [
    "Next, we're going to filter the data when the `PROVIDER` is `BSI` and the status is `Waiting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2b60630-7405-45ae-ae64-76e46cf65b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+------------+--------------------+--------+\n",
      "|ACCOUNT_NUMBER|PAYMENT_STATUS|PAYMENT_DATE|         EXPIRE_DATE|PROVIDER|\n",
      "+--------------+--------------+------------+--------------------+--------+\n",
      "|     043583173|       Waiting|        NULL|2020-03-25 03:35:...|     Ovo|\n",
      "|     021381685|       Waiting|        NULL|2017-10-12 03:15:...|     BSI|\n",
      "|     067955907|       Waiting|        NULL|2018-01-23 00:35:...|     Ovo|\n",
      "|     117651601|       Waiting|        NULL|2019-05-23 21:16:...|     BSI|\n",
      "|     117451542|       Waiting|        NULL|2016-10-09 18:40:...|     BSI|\n",
      "|     042230919|       Waiting|        NULL|2016-07-31 02:22:...|     BSI|\n",
      "|     071579340|       Waiting|        NULL|2018-05-28 17:52:...|     Ovo|\n",
      "|     041985553|       Waiting|        NULL|2015-08-12 03:03:...|     BSI|\n",
      "|     065955077|       Waiting|        NULL|2020-05-07 08:32:...|     Ovo|\n",
      "+--------------+--------------+------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_new_df = new_df.filter(\"PROVIDER in ('BSI','Ovo') and PAYMENT_STATUS = 'Waiting'\")\n",
    "filter_new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da495d4-1fbc-4d94-b6f6-a7befdae9bca",
   "metadata": {},
   "source": [
    "To save it only one file, we can use this snippet code\n",
    "\n",
    "```python\n",
    "df.coalesce(numPartitions = 1).write.csv(filename)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a7f9f-0e05-4651-8ea8-36250d7fa2de",
   "metadata": {},
   "source": [
    "---\n",
    "Say we want to save the output into directory `data/output/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd798f75-c32e-4180-a510-e588a26cf518",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.coalesce(numPartitions = 1).write.csv(\"../data/output/filtered_data_single\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef949658-ff1b-4d9c-b1cb-5dc40a0681a6",
   "metadata": {},
   "source": [
    "Then, if we want to save the output into partition files we can use without method `coalesce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84babc69-737e-44f5-bd95-5355847947d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.option(\"header\", True).csv(\"../data/output/filtered_data_partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565a880-0cb4-4aa5-9d84-4e2daa444582",
   "metadata": {},
   "source": [
    "**Save to Database**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5e154-2dfb-4d47-bc63-e37db95ea538",
   "metadata": {},
   "source": [
    "- To save to a database is just like when we read data from a database\n",
    "- We must config the connection first and use the `jdbc` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5197bd-c35f-4d8e-b733-cc2858350812",
   "metadata": {},
   "source": [
    "---\n",
    "- When load to the database, PySpark can generate the table automatically\n",
    "- But, you're also can build the table first and then load the data\n",
    "\n",
    "Schema Table\n",
    "\n",
    "```sqlrk;\n",
    "\n",
    "CREATE TABLE public.payment_pyspark (\n",
    "\t\"ACCOUNT_NUMBER\" text NULL,\n",
    "\t\"PAYMENT_STATUS\" varchar NULL,\n",
    "\t\"PAYMENT_DATE\" datetime NULL,\n",
    "\t\"EXPIRE_DATE\" datetime NULL,\n",
    "\t\"PROVIDER\" varchar NULL,\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8a7e786-ac38-4bb4-be4a-fe7272312d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable for database\n",
    "DB_URL = \"jdbc:postgresql://pachotel_db_container:5432/pachotel\"\n",
    "DB_TABLE = \"payment\" \n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"cobapassword\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40e1d402-792f-40e6-a831-1220b4e356bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"jdbc:postgresql://pachotel_db_container:5432/pachotel\"\n",
    "properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0189b153-2577-4c28-b769-112a862a7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.jdbc(url = url,\n",
    "                          table = \"payment_pyspark\",\n",
    "                          mode = \"overwrite\",\n",
    "                          properties = properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20032859-46ed-44e5-9470-66380f54faee",
   "metadata": {},
   "source": [
    "# <font color='blue'>Select Data using PySpark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14487e31-7f95-4be1-97c2-1c479d6d0594",
   "metadata": {},
   "source": [
    "- To Select Data using PySpark it's like in Pandas\n",
    "- To access the data that we want, we can select the columns that we want\n",
    "- Say, we want to access the flight transactional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81b3656f-47be-4eab-8301-1eebd7bf054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|Existing Customer|          45|     M|              3|    High School|       Married|    $60K - $80K|         Blue|            39|                       5|                     1|                    3|       12691|                777|          11914|               1.335|           1144|            42|              1.625|                0.061|\n",
      "|Existing Customer|          49|     F|              5|       Graduate|        Single| Less than $40K|         Blue|            44|                       6|                     1|                    2|        8256|                864|           7392|               1.541|           1291|            33|              3.714|                0.105|\n",
      "|Existing Customer|          51|     M|              3|       Graduate|       Married|   $80K - $120K|         Blue|            36|                       4|                     1|                    0|        3418|                  0|           3418|               2.594|           1887|            20|              2.333|                    0|\n",
      "|Existing Customer|          40|     F|              4|    High School|       Unknown| Less than $40K|         Blue|            34|                       3|                     4|                    1|        3313|               2517|            796|               1.405|           1171|            20|              2.333|                 0.76|\n",
      "|Existing Customer|          40|     M|              3|     Uneducated|       Married|    $60K - $80K|         Blue|            21|                       5|                     1|                    0|        4716|                  0|           4716|               2.175|            816|            28|                2.5|                    0|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_churns = spark \\\n",
    "            .read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(\"../data/bank_churners.csv\")\n",
    "\n",
    "df_churns.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505c452-f508-453d-9f17-9c5c5c840e10",
   "metadata": {},
   "source": [
    "- To select the columns, we can store them first in the Python list for easier use\n",
    "- Then, to select the data we can use syntax `select(col_1, col_2, col_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b4947fe-08dd-47d9-8a88-88dbb8748bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------------+---------------+--------------+---------------+\n",
      "|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|\n",
      "+------------+------+---------------+---------------+--------------+---------------+\n",
      "|          45|     M|              3|    High School|       Married|    $60K - $80K|\n",
      "|          49|     F|              5|       Graduate|        Single| Less than $40K|\n",
      "|          51|     M|              3|       Graduate|       Married|   $80K - $120K|\n",
      "|          40|     F|              4|    High School|       Unknown| Less than $40K|\n",
      "|          40|     M|              3|     Uneducated|       Married|    $60K - $80K|\n",
      "|          44|     M|              2|       Graduate|       Married|    $40K - $60K|\n",
      "|          32|     M|              0|    High School|       Unknown|    $60K - $80K|\n",
      "|          37|     M|              3|     Uneducated|        Single|    $60K - $80K|\n",
      "|          48|     M|              2|           NULL|        Single|   $80K - $120K|\n",
      "|          65|     M|              1|        Unknown|       Married|    $40K - $60K|\n",
      "|          56|     M|              1|        College|        Single|   $80K - $120K|\n",
      "|          35|     M|              3|       Graduate|       Unknown|    $60K - $80K|\n",
      "|          57|     F|              2|       Graduate|       Married| Less than $40K|\n",
      "|          44|     M|              4|        Unknown|       Unknown|   $80K - $120K|\n",
      "|          48|     M|              4|  Post-Graduate|        Single|   $80K - $120K|\n",
      "|          41|     M|              3|        Unknown|       Married|   $80K - $120K|\n",
      "|          61|     M|              1|    High School|       Married|    $40K - $60K|\n",
      "|          45|     F|              2|       Graduate|       Married|        Unknown|\n",
      "|          47|     M|              1|      Doctorate|      Divorced|    $60K - $80K|\n",
      "|          62|     F|              0|       Graduate|       Married| Less than $40K|\n",
      "+------------+------+---------------+---------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SELECTED_COLUMNS = [\"Customer_Age\",\"Gender\",\"Dependent_count\",\"Education_Level\",\"Marital_Status\",\"Income_Category\"]\n",
    "\n",
    "selected_df_churns = df_churns.select(SELECTED_COLUMNS)\n",
    "selected_df_churns.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b4d33-4d10-45aa-b504-e415607f819a",
   "metadata": {},
   "source": [
    "# <font color='blue'>Filter Data using PySpark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ef2a6-0d43-43dc-af0e-7b22e9750478",
   "metadata": {},
   "source": [
    "- To filter data in PySpark we can use the function `.filter()`\n",
    "- Also, when filtering in PySpark we can access via column or SQL based\n",
    "- So, when we're doing filtering data we can use **comparison operators**\n",
    "\n",
    "<center>\n",
    "\n",
    "| **Comparison Operators** | **Description**           |\n",
    "|--------------------------|-------------------------|\n",
    "| <                        | Less than             |\n",
    "| >                        | More than              |\n",
    "| <=                       | Less than equal to  |\n",
    "| >=                       | More than equal to  |\n",
    "| ==                       | Equal to             |\n",
    "| !=                       | Not equal to       |\n",
    "\n",
    "</center>\n",
    "\n",
    "- When filtering data in PySpark we can do single or multiple filter\n",
    "- When do multiple filtering we can use **boolean logic** like **and (`&`)** **or (`|`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf31ac84-4c48-49be-a4c0-672feb35829d",
   "metadata": {},
   "source": [
    "**Single Filter**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3eda1-9920-4f1c-9b9c-4a448a6e92d4",
   "metadata": {},
   "source": [
    "When filtering only one column we can use this snippet code\n",
    "\n",
    "```python\n",
    "df_filtered = df.filter(df[col_1] > value)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c3fa5ec-4048-4c35-9579-dc9e1df9d4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|Existing Customer|          49|     F|              5|       Graduate|        Single| Less than $40K|         Blue|            44|                       6|                     1|                    2|        8256|                864|           7392|               1.541|           1291|            33|              3.714|                0.105|\n",
      "|Existing Customer|          40|     F|              4|    High School|       Unknown| Less than $40K|         Blue|            34|                       3|                     4|                    1|        3313|               2517|            796|               1.405|           1171|            20|              2.333|                 0.76|\n",
      "|Existing Customer|          57|     F|              2|       Graduate|       Married| Less than $40K|         Blue|            48|                       5|                     2|                    2|        2436|                680|           1756|                1.19|           1570|            29|              0.611|                0.279|\n",
      "|Existing Customer|          45|     F|              2|       Graduate|       Married|        Unknown|         Blue|            37|                       6|                     1|                    2|       14470|               1157|          13313|               0.966|           1207|            21|              0.909|                 0.08|\n",
      "|Attrited Customer|          62|     F|              0|       Graduate|       Married| Less than $40K|         Blue|            49|                       2|                     3|                    3|      1438.3|                  0|         1438.3|               1.047|            692|            16|                0.6|                    0|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df_churns.filter(df_churns['Gender'] =='F')\n",
    "df_filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6426c-28a6-44e9-93b7-788eaa45db10",
   "metadata": {},
   "source": [
    "**Multiple Filter**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12529afa-5b56-468e-a463-478b25da385f",
   "metadata": {},
   "source": [
    "When do multiple filtering in PySpark, we can use **boolean logic**\n",
    "\n",
    "```python\n",
    "\n",
    "# using columns based\n",
    "filtered_df = df.filter((df[col_1] > value_1) & (df[col_2] == value_2))\n",
    "\n",
    "# using sql based\n",
    "filtered_df = df.filter(\"col_1 > value_1 AND col_2 = value_2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "220e3aca-724a-450c-bc99-e2efcff803a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|Existing Customer|          51|     M|              3|       Graduate|       Married|   $80K - $120K|         Blue|            36|                       4|                     1|                    0|        3418|                  0|           3418|               2.594|           1887|            20|              2.333|                    0|\n",
      "|Existing Customer|          44|     M|              2|       Graduate|       Married|    $40K - $60K|         Blue|            36|                       3|                     1|                    2|        4010|               1247|           2763|               1.376|           1088|            24|              0.846|                0.311|\n",
      "|Existing Customer|          35|     M|              3|       Graduate|       Unknown|    $60K - $80K|         Blue|            30|                       5|                     1|                    3|        8547|               1666|           6881|               1.163|           1311|            33|                  2|                0.195|\n",
      "|Existing Customer|          41|     M|              4|       Graduate|       Married|    $60K - $80K|         Blue|            36|                       4|                     1|                    2|        8923|               2517|           6406|               1.726|           1589|            24|              1.667|                0.282|\n",
      "|Existing Customer|          58|     M|              0|       Graduate|       Married|   $80K - $120K|         Blue|            49|                       6|                     2|                    2|       12555|               1696|          10859|               0.519|           1291|            24|              0.714|                0.135|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df_churns.filter((df_churns['Gender'] =='M')& (df_churns['Education_Level'] =='Graduate'))\n",
    "df_filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53a269-05d4-4b2c-8f41-bf21a18fa551",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Transformation using PySpark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439cd07-117f-4904-bd03-f7f75074e940",
   "metadata": {},
   "source": [
    "- In this section, we're going to learn how to do Data Wrangling or Data Transformation in PySpark\n",
    "- Just like in Pandas, we can also do Data Transformation using PySpark\n",
    "- There are so many tasks that we can do in Data Transformation by using PySpark\n",
    "- But, in this session we will focus to:\n",
    "    - Rename Column\n",
    "    - Slicing Data\n",
    "    - Create a New Column using the Existing Column\n",
    "    - Impute Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680e45a-031e-44ed-a790-cdae493b26ec",
   "metadata": {},
   "source": [
    "Just like in the previous section, make sure we've already created `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b04d187-3439-44ea-a053-ee576e58f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ad786d-c8d3-4a7f-b773-e75ca04f757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data Transformation using PySpark\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ffc8c7-5b16-45c0-b303-5bd7d6e3c400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data Transformation using PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5eb435c690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94916b3a-1cc6-4bfc-ba18-a654f5dd3724",
   "metadata": {},
   "source": [
    "- We will be using data from the previous section\n",
    "- The data is in this directory `data/materi_10/flights_filtered.csv`\n",
    "- We can also create a Python function to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23e2a8a-b8c4-4dfe-b2d1-5d88429f58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data(filename: str) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"Function to read csv file using PySpark\"\"\"\n",
    "    df = spark \\\n",
    "        .read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(filename)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b5d187-1246-490e-a583-b86719bb0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data path\n",
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c2ca52b-7642-4c5b-bdd9-cebe67bb962d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|Existing Customer|          45|     M|              3|    High School|       Married|    $60K - $80K|         Blue|            39|                       5|                     1|                    3|       12691|                777|          11914|               1.335|           1144|            42|              1.625|                0.061|\n",
      "|Existing Customer|          49|     F|              5|       Graduate|        Single| Less than $40K|         Blue|            44|                       6|                     1|                    2|        8256|                864|           7392|               1.541|           1291|            33|              3.714|                0.105|\n",
      "|Existing Customer|          51|     M|              3|       Graduate|       Married|   $80K - $120K|         Blue|            36|                       4|                     1|                    0|        3418|                  0|           3418|               2.594|           1887|            20|              2.333|                    0|\n",
      "|Existing Customer|          40|     F|              4|    High School|       Unknown| Less than $40K|         Blue|            34|                       3|                     4|                    1|        3313|               2517|            796|               1.405|           1171|            20|              2.333|                 0.76|\n",
      "|Existing Customer|          40|     M|              3|     Uneducated|       Married|    $60K - $80K|         Blue|            21|                       5|                     1|                    0|        4716|                  0|           4716|               2.175|            816|            28|                2.5|                    0|\n",
      "+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data = read_csv_data(DATA_PATH + \"bank_churners.csv\")\n",
    "\n",
    "df_data.show(5) # get 5 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af08d63-1bf6-42a5-92e0-ef953485ce34",
   "metadata": {},
   "source": [
    "### **Rename Column**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1197c-c76a-479b-9fcb-6cf7505c5b9f",
   "metadata": {},
   "source": [
    "- When reading data directly from data sources, usually there's a problem\n",
    "- The naming convention for column names is not proper\n",
    "- For naming convention usually each person may differ, we can use camelCase or snake_case\n",
    "- In this course, we will use snake_case\n",
    "- To rename column names in PySpark we can use two methods:\n",
    "    - `withColumnRenamed()`: can be used when rename one column only\n",
    "    - `withColumnsRenamed()`: can be used when rename multiple columns\n",
    "\n",
    "**Syntax Rename One Column**\n",
    "\n",
    "```python\n",
    "df = df.withColumnRenamed(existing = old_column, new = new_column)\n",
    "```\n",
    "\n",
    "**Syntax Rename Multiple Columns**\n",
    "\n",
    "```python\n",
    "# initiate dictionary\n",
    "RENAME_COLS = {\n",
    "    \"old_col_1\": \"new_col_1\",\n",
    "    \"old_col_2\": \"new_col_2\",\n",
    "    \"old_col_n\": \"new_col_n\"\n",
    "}\n",
    "\n",
    "df = df.withColumnsRenamed(colsMap = RENAME_COLS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec92ce3-f33c-49fe-ac73-6bbbec04dc26",
   "metadata": {},
   "source": [
    "**Rename One Column**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e25a9-a093-45f9-b09d-74907fcac07c",
   "metadata": {},
   "source": [
    "By using data in variable `df_fligts`, we want to rename column `AIRLINE` to `airline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdb5c6-eac2-42c2-bbb7-8f45d330f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights.withColumnRenamed(existing = \"Dependent_count\", new = \"Dependent_Count\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
